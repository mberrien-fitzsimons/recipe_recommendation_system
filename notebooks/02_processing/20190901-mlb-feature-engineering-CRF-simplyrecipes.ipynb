{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Simply Recipes Ingredients for CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/markishab/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pycrfsuite\n",
    "import numpy as np\n",
    "import re\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYT CRF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_ing = pd.read_csv('../../data/01_raw/nyt-ingredients-snapshot-2015.csv')\n",
    "nyt_ing.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>name</th>\n",
       "      <th>qty</th>\n",
       "      <th>range_end</th>\n",
       "      <th>unit</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 1/4 cups cooked and pureed fresh butternut s...</td>\n",
       "      <td>butternut squash</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cup</td>\n",
       "      <td>cooked and pureed fresh, or 1 10-ounce package...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 cup peeled and cooked fresh chestnuts (about...</td>\n",
       "      <td>chestnuts</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cup</td>\n",
       "      <td>peeled and cooked fresh (about 20), or 1 cup c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 medium-size onion, peeled and chopped</td>\n",
       "      <td>onion</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medium-size, peeled and chopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 stalks celery, chopped coarse</td>\n",
       "      <td>celery</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stalk</td>\n",
       "      <td>chopped coarse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 1/2 tablespoons vegetable oil</td>\n",
       "      <td>vegetable oil</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tablespoon</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input              name   qty  \\\n",
       "0  1 1/4 cups cooked and pureed fresh butternut s...  butternut squash  1.25   \n",
       "1  1 cup peeled and cooked fresh chestnuts (about...         chestnuts  1.00   \n",
       "2            1 medium-size onion, peeled and chopped             onion  1.00   \n",
       "3                    2 stalks celery, chopped coarse            celery  2.00   \n",
       "4                    1 1/2 tablespoons vegetable oil     vegetable oil  1.50   \n",
       "\n",
       "   range_end        unit                                            comment  \n",
       "0        0.0         cup  cooked and pureed fresh, or 1 10-ounce package...  \n",
       "1        0.0         cup  peeled and cooked fresh (about 20), or 1 cup c...  \n",
       "2        0.0         NaN                    medium-size, peeled and chopped  \n",
       "3        0.0       stalk                                     chopped coarse  \n",
       "4        0.0  tablespoon                                                NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_ing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_ing.fillna(\"missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make ingredients list into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingredients_list = list(nyt_ing.input)\n",
    "# ingredients_list_new = []\n",
    "# for ingredient in ingredients_list:\n",
    "#     try: \n",
    "#         ing_update = re.sub(r'(\\d+)\\s+(\\d)/(\\d)', r'\\1$\\2/\\3', ingredient)\n",
    "#         ingredients_list_new.append(ing_update.split(\" \"))\n",
    "#     except:\n",
    "#         ingredients_list_new.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a dictionary of the nyt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_list = list(nyt_ing.input)\n",
    "# name_list = list(nyt_ing.name)\n",
    "# qty_list = list(nyt_ing.qty)\n",
    "# unit_list = list(nyt_ing.unit)\n",
    "# comment_list = list(nyt_ing.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's add the dollar sign to the input list\n",
    "# input_list_new = []\n",
    "# for i in ingredients_list_new:\n",
    "#     try:\n",
    "#         input_list_new.append(re.sub(r'(\\d+)\\s+(\\d)/(\\d)', r'\\1$\\2/\\3', inp))\n",
    "#     except:\n",
    "#         input_list_new.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def singularize(word):\n",
    "#     \"\"\"\n",
    "#     A poor replacement for the pattern.en singularize function, but ok for now.\n",
    "#     \"\"\"\n",
    "\n",
    "#     units = {\n",
    "#         \"cups\": u\"cup\",\n",
    "#         \"tablespoons\": u\"tablespoon\",\n",
    "#         \"teaspoons\": u\"teaspoon\",\n",
    "#         \"pounds\": u\"pound\",\n",
    "#         \"ounces\": u\"ounce\",\n",
    "#         \"cloves\": u\"clove\",\n",
    "#         \"sprigs\": u\"sprig\",\n",
    "#         \"pinches\": u\"pinch\",\n",
    "#         \"bunches\": u\"bunch\",\n",
    "#         \"slices\": u\"slice\",\n",
    "#         \"grams\": u\"gram\",\n",
    "#         \"heads\": u\"head\",\n",
    "#         \"quarts\": u\"quart\",\n",
    "#         \"stalks\": u\"stalk\",\n",
    "#         \"pints\": u\"pint\",\n",
    "#         \"pieces\": u\"piece\",\n",
    "#         \"sticks\": u\"stick\",\n",
    "#         \"dashes\": u\"dash\",\n",
    "#         \"fillets\": u\"fillet\",\n",
    "#         \"cans\": u\"can\",\n",
    "#         \"ears\": u\"ear\",\n",
    "#         \"packages\": u\"package\",\n",
    "#         \"strips\": u\"strip\",\n",
    "#         \"bulbs\": u\"bulb\",\n",
    "#         \"bottles\": u\"bottle\"\n",
    "#     }\n",
    "\n",
    "#     if word in units.keys():\n",
    "#         return units[word]\n",
    "#     else:\n",
    "#         return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can't make the name, quantity and comment match up with the right word. let's reevaluation what I'm doing. Do I really need to do this this way? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put data in correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_ing_tuple = []\n",
    "for index, row in nyt_ing.iterrows():\n",
    "    name = row[1]\n",
    "    qty = row[2]\n",
    "    unit = row[4]\n",
    "    comment = row[5]\n",
    "    nyt_ing_tuple.append([(qty, 'qty'), (unit, 'unit'), (name, 'name'), (comment, 'comment')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break up the tuples with multiple words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_ing_tuple_new = []\n",
    "for sub_list in nyt_ing_tuple: \n",
    "    sub_ls = []\n",
    "    for elem in sub_list:\n",
    "        if \" \" in str(elem[0]):\n",
    "            elem2 = elem[0].split(\" \")\n",
    "            for el in elem2:\n",
    "                sub_ls.append((el, elem[1]))\n",
    "        else: \n",
    "            sub_ls.append((elem[0], elem[1]))\n",
    "    nyt_ing_tuple_new.append(sub_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's erase the tuples with missing as a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_big, ingredient in enumerate(nyt_ing_tuple_new):\n",
    "    for idx, word in enumerate(ingredient):\n",
    "        if word[0] == 'missing':\n",
    "            nyt_ing_tuple_new[idx_big].remove(word)\n",
    "        if word[0] == '':\n",
    "            nyt_ing_tuple_new[idx_big].remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip all punctuation from the tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_data = []\n",
    "for sub_list in nyt_ing_tuple_new:\n",
    "    sublist = []\n",
    "    for word in sub_list:\n",
    "        word2 = str(word[0]).strip(')!,.?(')\n",
    "        sublist.append((word2, word[1]))\n",
    "    crf_data.append(sublist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GREAT, we've got the data in the way that we want. let's construct some features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4.0', 'qty'),\n",
       " ('tablespoon', 'unit'),\n",
       " ('butter', 'name'),\n",
       " ('', 'comment'),\n",
       " ('1/2', 'comment'),\n",
       " ('stick),', 'comment'),\n",
       " ('melted', 'comment')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_data[1664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b8d8a81f5eae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Take the word, POS tag, and its label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdata_nyt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-b8d8a81f5eae>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Take the word, POS tag, and its label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdata_nyt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "data_nyt = []\n",
    "for i, doc in enumerate(crf_data):\n",
    "\n",
    "    # Obtain the list of tokens in the document\n",
    "    tokens = [t for t, label in doc]\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    try:\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "    except:\n",
    "        tagged = 'missing'\n",
    "\n",
    "    # Take the word, POS tag, and its label\n",
    "    data_nyt.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1664"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_nyt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lengthGroup(actualLength):\n",
    "    \"\"\"\n",
    "    Buckets the length of the ingredient into 6 buckets.\n",
    "    \"\"\"\n",
    "    for n in [4, 8, 12, 16, 20]:\n",
    "        if actualLength < n:\n",
    "            return str(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    postag = doc[i][1]\n",
    "\n",
    "    # Common features for all words\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        \"L%s\" % lengthGroup(length),\n",
    "        'postag=' + postag\n",
    "    ]\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the beginning of a document\n",
    "    if i > 0:\n",
    "        word1 = doc[i-1][0]\n",
    "        postag1 = doc[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '-1:word[-3:]=' + word[-3:],\n",
    "            '-1:word[-2:]=' + word[-2:],\n",
    "            \"L%s\" % lengthGroup(length),\n",
    "            '-1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features.append('BOS')\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the end of a document\n",
    "    if i < len(doc)-1:\n",
    "        word1 = doc[i+1][0]\n",
    "        postag1 = doc[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word[-3:]=' + word[-3:],\n",
    "            '+1:word[-2:]=' + word[-2:],\n",
    "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "            \"L%s\" % lengthGroup(length),\n",
    "            '+1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF TUTORIAL: Performing Sequence Labelling using CRF in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4.element import Tag\n",
    "import codecs\n",
    "\n",
    "# Read data file and parse the XML\n",
    "with codecs.open(\"../../data/tutorials/reuters.xml\", \"r\", \"utf-8\") as infile:\n",
    "    soup = bs(infile, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put Data in form that CRF likes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for elem in soup.find_all(\"document\"):\n",
    "    texts = []\n",
    "\n",
    "    # Loop through each child of the element under \"textwithnamedentities\"\n",
    "    for c in elem.find(\"textwithnamedentities\").children:\n",
    "        if type(c) == Tag:\n",
    "            if c.name == \"namedentityintext\":\n",
    "                label = \"N\"  # part of a named entity\n",
    "            else:\n",
    "                label = \"I\"  # irrelevant word\n",
    "            for w in c.text.split(\" \"):\n",
    "                if len(w) > 0:\n",
    "                    texts.append((w, label))\n",
    "    docs.append(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, doc in enumerate(docs):\n",
    "\n",
    "    # Obtain the list of tokens in the document\n",
    "    tokens = [t for t, label in doc]\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    print(tagged)\n",
    "\n",
    "#     # Take the word, POS tag, and its label\n",
    "#     data.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    postag = doc[i][1]\n",
    "\n",
    "    # Common features for all words\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag\n",
    "    ]\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the beginning of a document\n",
    "    if i > 0:\n",
    "        word1 = doc[i-1][0]\n",
    "        postag1 = doc[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '-1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features.append('BOS')\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the end of a document\n",
    "    if i < len(doc)-1:\n",
    "        word1 = doc[i+1][0]\n",
    "        postag1 = doc[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '+1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# A function for extracting features in documents\n",
    "def extract_features(doc):\n",
    "    return [word2features(doc, i) for i in range(len(doc))]\n",
    "\n",
    "# A function fo generating the list of labels for each document\n",
    "def get_labels(doc):\n",
    "    return [label for (token, postag, label) in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [extract_features(doc) for doc in data]\n",
    "y = [get_labels(doc) for doc in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=True)\n",
    "\n",
    "# Submit training data to the trainer\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "\n",
    "# Set the parameters of the model\n",
    "trainer.set_params({\n",
    "    # coefficient for L1 penalty\n",
    "    'c1': 0.1,\n",
    "\n",
    "    # coefficient for L2 penalty\n",
    "    'c2': 0.01,  \n",
    "\n",
    "    # maximum number of iterations\n",
    "    'max_iterations': 200,\n",
    "\n",
    "    # whether to include transitions that\n",
    "    # are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "# Provide a file name as a parameter to the train function, such that\n",
    "# the model will be saved to the file when training is finished\n",
    "trainer.train('crf.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('crf.model')\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "\n",
    "# Let's take a look at a random sample in the testing set\n",
    "i = 12\n",
    "for x, y in zip(y_pred[i], [x[1].split(\"=\")[1] for x in X_test[i]]):\n",
    "    print(\"%s (%s)\" % (y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOURCE:**\n",
    "* <font color='red'>Performing Sequence Labelling using CRF in Python</font>\n",
    "* https://eli5.readthedocs.io/en/latest/tutorials/sklearn_crfsuite.html\n",
    "* https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.crf\n",
    "* http://www.nltk.org/book/ch00.html\n",
    "* https://python-crfsuite.readthedocs.io/en/latest/\n",
    "* https://open.nytimes.com/\n",
    "* CRF Suite Tutorial: http://www.chokkan.org/software/crfsuite/tutorial.html\n",
    "* sklearn_crfsuite tutorial: https://eli5.readthedocs.io/en/latest/tutorials/sklearn_crfsuite.html\n",
    "* NYT Ingredients Parser: https://github.com/nytimes/ingredient-phrase-tagger\n",
    "* https://github.com/kulsoom-abdullah/kulsoom-abdullah.github.io/tree/master/AWS-lambda-implementation\n",
    "* End to End Recipe Cuisine Classification: https://towardsdatascience.com/https-towardsdatascience-com-end-to-end-recipe-cuisine-classification-e97f4ac22104\n",
    "* Performing Sequence Labelling using CRF in Python: http://www.albertauyeung.com/post/python-sequence-labelling-with-crf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
